{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761224d1-e5fa-4efa-a241-8f7dad0ccc61",
   "metadata": {},
   "source": [
    "## process a synthetic chorale manufactured by TonicNet\n",
    "TonicNet can produce a synthetic chorale using the main.py module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7718221-3877-4386-bf09-32cb2e320ff3",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 5.1M\n",
      "drwxr-xr-x. 1 prent prent 139K Mar  8 11:03 numpy_chorales\n",
      "drwxr-xr-x. 1 prent prent 130K Mar  8 11:03 midi_samples\n",
      "-rw-r--r--. 1 prent prent  158 Mar  7 15:26 sample_smoothed.mid\n",
      "drwxr-xr-x. 1 prent prent  246 Mar  5 10:18 __pycache__\n",
      "drwxr-xr-x. 1 prent prent   66 Jan 25 09:04 samples\n",
      "-rw-r--r--. 1 prent prent 205K Jul 15  2022 TonicNet_epoch-58_loss-0.317_acc-90.928.pt\n",
      "-rw-r--r--. 1 prent prent 4.9M Jul 14  2022 TonicNet_epoch-58_loss-0.322_acc-90.745.pt\n",
      "-rw-r--r--. 1 prent prent 7.7K Jul 13  2022 sample.py\n",
      "-rw-r--r--. 1 prent prent 3.1K Jul 12  2022 eval.py\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "!ls -lth ./eval | head \n",
    "print(f'Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0c6190-a10e-4b50-82c1-4db467314b0c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTZ5xE7jaVy0",
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import mido\n",
    "import time\n",
    "from importlib import reload\n",
    "import fluidsynth\n",
    "from IPython.display import Audio, display\n",
    "import music21 as m\n",
    "import os\n",
    "import muspy\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '/home/prent/Dropbox/Tutorials/coconet-pytorch/coconet-pytorch-csound')\n",
    "# import piano as p\n",
    "# import selective_stretching_codes as stretch\n",
    "import samples_used as su\n",
    "import subprocess\n",
    "from numpy.random import default_rng\n",
    "rng = np.random.default_rng()\n",
    "soundfont = '../coconet-pytorch/font.sf2' # you will need to download this from location specified in the github README.md\n",
    "midi_dir = 'eval/midi_samples'\n",
    "numpy_dir = 'eval/numpy_chorales'\n",
    "CSD_FILE = 'string_orc.csd'\n",
    "LOGNAME = 'string_orc.log'\n",
    "WAV_OUT = '/home/prent/Music/sflib/string_orc.wav'\n",
    "model_path = 'eval/TonicNet_epoch-58_loss-0.322_acc-90.745.pt'\n",
    "downbeat = 1 # all the synthetic chorales out of TonicNet have a downbeat of 1\n",
    "keys = ['C','C#','D','D#','E','F','F#','G','G#','A','B-','B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c76dc5-32bb-4ab8-aa42-41a64bd41cb7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prent/Dropbox/Tutorials/TonicNet\n",
      "-rw-r--r--. 1 prent prent 4.9M Jul 14  2022 eval/TonicNet_epoch-58_loss-0.322_acc-90.745.pt\n",
      "loded params from eval/TonicNet_epoch-58_loss-0.322_acc-90.745.pt\n",
      "TonicNet(\n",
      "  (embedding): Embedding(98, 256)\n",
      "  (pos_emb): Embedding(64, 0)\n",
      "  (z_embedding): Embedding(80, 32)\n",
      "  (dropout_i): VariationalDropout()\n",
      "  (rnn): GRU(288, 256, num_layers=3, batch_first=True)\n",
      "  (dropout_o): VariationalDropout()\n",
      "  (hidden_to_tag): Linear(in_features=288, out_features=98, bias=False)\n",
      ")\n",
      "\n",
      "0\n",
      "\t 0 : (10, 'major')\n",
      "ending\n",
      "SAVED sample to ./eval/sample.mid\n",
      "/home/prent/Dropbox/Tutorials/TonicNet/eval/utils.py:120: StreamIteratorInefficientWarning: flat is not defined on StreamIterators. Call .stream() first for efficiency\n",
      "  for n in part.notesAndRests.flat:\n",
      "SAVED rhythmically 'smoothed' sample to ./eval/sample_smoothed.mid\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls -lth eval/TonicNet_epoch-58_loss-0.322_acc-90.745.pt\n",
    "!python main.py -s\n",
    "# the \"smoothed\" algorithm doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcea806-b6bf-4f37-a713-970938259d1b",
   "metadata": {},
   "source": [
    "## Use the TonicNet sample generator to create and save synthetic chorales as numpy files\n",
    "Steps:\n",
    "- generate a sythetic chorale as a music21 stream.\n",
    "- convert that to the chorale format, (notes, voices) by time-interval in piano_roll format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21abccc-3023-4c42-9fe2-ba501568389c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from eval.sample import sample_TonicNet_random\n",
    "from eval.utils import plot_loss_acc_curves, indices_to_stream, smooth_rhythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81173085-b97e-4a7b-9871-766f3c8df417",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def midi_to_input(midi_file):\n",
    "    music = muspy.read(midi_file)\n",
    "    if music.key_signatures != []: # check if the midi file includes a key signature - some don't\n",
    "        root = music.key_signatures[0].root \n",
    "        mode = music.key_signatures[0].mode # major or minor\n",
    "    else: \n",
    "        print('Warning: no key signature found. Assuming C major')\n",
    "        mode = \"major\"\n",
    "        root = 0    \n",
    "    if music.time_signatures != []: # check if the midi file includes a time signature - some don't\n",
    "        numerator = music.time_signatures[0].numerator\n",
    "        denominator = music.time_signatures[0].denominator \n",
    "    else: \n",
    "        print('Warning: no time signature found. Assuming 4/4')\n",
    "        numerator = 4\n",
    "        denominator = 4\n",
    "    # turn it into a piano roll\n",
    "    piano_roll = muspy.to_pianoroll_representation(music,encode_velocity=False) # boolean piano roll if False, default True\n",
    "    # print(piano_roll.shape) # should be one time step for every click in the midi file\n",
    "    q = music.resolution # quarter note value in this midi file. \n",
    "    q16 = q // 4 # my desired resolution is by 1/16th notes\n",
    "    print(f'time signatures: {numerator}/{denominator}')\n",
    "    time_steps = piano_roll.shape[0] // q16\n",
    "    print(f'music.resolution is q: {q}. q16: {q16} time_steps: {time_steps} 1/16th notes')\n",
    "    sample= np.zeros(shape=(time_steps,4)).astype(int) # default is float unless .astype(int)\n",
    "    # This loop is able to load an array of shape N,4 with the notes that are being played in each time step\n",
    "    for click in range(0,piano_roll.shape[0],q16): # q16 is skip 240 steps for 1/16th note resolution\n",
    "        voice = 3 # start with the low voices and decrement for the higher voices as notes get higher\n",
    "        for i in range(piano_roll.shape[1]): # check if any notes are non-zero\n",
    "              time_interval = (click) // q16 \n",
    "              if (piano_roll[click][i]): # if velocity anything but zero - unless you set encode_velocity = False\n",
    "              # if time_interval % 16 == 0:\n",
    "              #     print(f'time step: {click} at index {i}, time_interval: {time_interval}, voice: {voice}')\n",
    "              # i is the midi note number. I want to transpose it into C\n",
    "                    sample[time_interval][voice] = i - root # index to the piano roll with a note - transposed by the key if not C which is 0\n",
    "                    voice -= 1 # next instrument will get the higher note\n",
    "    return (sample,root,mode)           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d709d4f0-fad5-4738-a0d1-b68e777f270b",
   "metadata": {},
   "source": [
    "## Manufacture synthetic chorales in bulk\n",
    "\n",
    "- Call the TonicNet synthesize function <code>sample_TonicNet_random(load_path = model_path, temperature=1.0)</code>\n",
    "- This creates a MIDI file called <code>sample.mid</code> in the <code>eval</code> directory\n",
    "- Load that MIDI file into a chorale structure of (voices, time_steps)\n",
    "- Save that chorale as a numpy file\n",
    "- Rename the <code>sample.mid</code> in the <code>eval</code> directory as <code>sample#.mid</code> where # is a number of the chorale in the range\n",
    "- Read the stream into a Music21 data structure so that we can determine the key and the mode of the chorale\n",
    "- At the end of the process, the <code>eval</code> directory has all the MIDI synthetic chorales, and the <code>eval/numpy_chorales</code> as all the numpy arrays of chorales in (voices, time_step) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb689f4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midi_dir ='eval/midi_samples', numpy_dir = 'eval/numpy_chorales', sample_name = 'eval/sample.mid'\n"
     ]
    }
   ],
   "source": [
    "sample_name = os.path.join('eval','sample.mid')\n",
    "print(f'{midi_dir =}, {numpy_dir = }, {sample_name = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c205f2ca-ab53-4373-b430-90f1062ee95a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loded params from eval/TonicNet_epoch-58_loss-0.322_acc-90.745.pt\n",
      "TonicNet(\n",
      "  (embedding): Embedding(98, 256)\n",
      "  (pos_emb): Embedding(64, 0)\n",
      "  (z_embedding): Embedding(80, 32)\n",
      "  (dropout_i): VariationalDropout()\n",
      "  (rnn): GRU(288, 256, num_layers=3, batch_first=True)\n",
      "  (dropout_o): VariationalDropout()\n",
      "  (hidden_to_tag): Linear(in_features=288, out_features=98, bias=False)\n",
      ")\n",
      "\n",
      "0\n",
      "\t 0 : (3, 'major')\n",
      "ending\n",
      "SAVED sample to ./eval/sample.mid\n",
      "about to read in sample_name = 'eval/sample.mid'\n",
      "Warning: no key signature found. Assuming C major\n",
      "time signatures: 4/4\n",
      "music.resolution is q: 1024. q16: 256 time_steps: 263 1/16th notes\n",
      "moving sample_name = 'eval/sample.mid' to output_name = 'eval/midi_samples/sample5000.mid'\n",
      "music21 says: fis = 'C major'\n",
      "after split. key_name = 'C', mode = 'major'\n",
      "found a match between music21 key_name = 'C', and key = 'C' as root = 0, mode = 'major'\n"
     ]
    }
   ],
   "source": [
    "# Synthesize a number of chorales as MIDI files and numpy files\n",
    "for synth_chorale in range(5000, 5001): # \n",
    "    # generate a synthetic chorale as a tensor\n",
    "    x = sample_TonicNet_random(load_path = model_path, temperature=1.0)\n",
    "    # create a sample.mid file in the eval directory in the same directory as the model_path\n",
    "    indices_to_stream(x, return_stream = False)\n",
    "    print(f'about to read in {sample_name = }')\n",
    "    sample, _, _ = midi_to_input(sample_name) # sample its notes, voices\n",
    "    chorale = sample.transpose()\n",
    "    numpy_file = os.path.join(numpy_dir, 'chorale' + str(synth_chorale))\n",
    "    np.save(numpy_file, chorale)\n",
    "    output_name = os.path.join(midi_dir, 'sample' + str(synth_chorale) + '.mid')\n",
    "    print(f'moving {sample_name = } to {output_name = }')\n",
    "    os.rename(sample_name, output_name)\n",
    "    m21_stream = indices_to_stream(x, return_stream = True) # this converts the tensor to a music21 stream for analysis\n",
    "    fis = str(m21_stream.analyze('key'))\n",
    "    print(f'music21 says: {fis = }')\n",
    "    key_name, mode = fis.split()\n",
    "    print(f'after split. {key_name = }, {mode = }')\n",
    "    i = 0\n",
    "    for key in keys:\n",
    "        if key.upper() == key_name.upper():\n",
    "            root = i\n",
    "            print(f'found a match between music21 {key_name = }, and {key = } as {root = }, {mode = }')\n",
    "        i += 1    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d8ab29-4506-4717-b1a4-290c2dd7c874",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no key signature found. Assuming C major\n",
      "time signatures: 4/4\n",
      "music.resolution is q: 1024. q16: 256 time_steps: 504 1/16th notes\n",
      "[68 63 60 53]\n",
      "[68 63 60 53]\n",
      "[68 63 60 53]\n",
      "[68 63 60 53]\n",
      "[70 63 58 55]\n",
      "[70 63 58 55]\n",
      "[70 63 58 55]\n",
      "[70 63 58 55]\n",
      "[ 0 71 63 56]\n",
      "[ 0 71 63 56]\n"
     ]
    }
   ],
   "source": [
    "synth_chorale = 4888\n",
    "file_name = 'eval/midi_samples/sample' + str(synth_chorale) + '.mid'\n",
    "sample, root, mode = midi_to_input(file_name) # sample is notes, voices by time interval\n",
    "print(*sample[:10],sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10550e-50dc-47be-a0c1-3d0a19accfc1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
